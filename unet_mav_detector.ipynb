{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a987218f-62bb-4326-a4bd-e127aeb794f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as pydata\n",
    "import torchmetrics as metrics\n",
    "\n",
    "from matplotlib.pylab import plt\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d766f6-6edd-4e1f-b95c-da4f9a0ecf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sklearn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.23.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34505837-07d2-4ce9-b327-1a431136969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maverick = pd.read_csv(f\"./T_vaginalis_G3.mavericks_for_ML.txt\", delimiter=\"\\t\")\n",
    "\n",
    "full_data = pd.read_csv(f\"./T_vaginalis_G3.genome.sequence.3column.txt\",delimiter=\"\\t\",names=[\"chromosome\",\"sequence\",\"length\"])\n",
    "full_data['chromosome'] = full_data['chromosome'].apply(lambda x : x.strip()[1:])\n",
    "\n",
    "# Filter out any chromosomes larger than 25million (temporary - data is too large)\n",
    "full_data = full_data[full_data['length'] >= 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ffc152-9686-4200-843f-804457cbbc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    chromosome                                           sequence    length\n",
      "0       chr_IV  CATGCTAATGGAATGCGTCGTTACCCACCAGTAAAAGTATTTGTGA...  40211917\n",
      "1        chr_V  TAGTGAACGACTTCTCATCTGGAAAGAAACTGAAGTCTGAAGGTTT...  34657175\n",
      "2       chr_II  TTCCCAGAAACAGACTTAGAACAAATTCCCCTTTCTGTTACACTAT...  26081621\n",
      "3      chr_III  GCTACTGTTTTCGAAATAAAAAGAGTAAAAACAAATTTTTATAACT...  27737965\n",
      "4       chr_VI  TATTTTCCCTATATCATTCGTAAATTTTCTTTCTATATTCTTCAAG...  20331443\n",
      "5        chr_I  TTCAAAAATTTTCCCAAGAAGAAAAAATAATTAGGAAATTTAATAT...  27726287\n",
      "44      ctg_44  CCTGTTTTTTCACCCTCGTGACTTTGTCATAACTTCTTCGTAAATG...    109354\n",
      "86      ctg_87  TTTAACCTTTTATCTTCACCGAGTTCATCCGAAGATTGAACGTTAA...    357916\n",
      "87      ctg_88  TATTCGTGGAGTTATGGGCCATTAAAAAAAAGGAGGATTACGACCC...    114971\n",
      "113    ctg_114  ACATTGCTGGTAGTTCATATGCTTCAAAGTTTCTGACATGACCTTC...    312855\n",
      "127    ctg_128  TTCGAGGTATAAAGTTCTATACACCTTCTCCAAACTTCTATTCTAT...    223072\n",
      "155    ctg_156  GAGGTATAGTTCGAATACGAGTATAATATTTTTCCTGCCCAGTTGG...    233009\n",
      "163    ctg_164  TCCATCATAGTACGTTCCATAACACTATGGTAACAAAATCTATTAC...    156932\n",
      "169    ctg_170  AAAAGAACCACCAAAACCAACAGCTAATGAAATATTGTTCGAATAT...    170613\n",
      "171    ctg_172  GAAAACAGTGAAGAACAGCGAGTACGTAAAAAAACGTAGTCGCCAA...    305720\n",
      "185    ctg_186  GGTACTGAACGTCTATGATATAAGTCAATAGCAATGTCATTTAAAG...    236109\n"
     ]
    }
   ],
   "source": [
    "print(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e99032-a23f-4c94-a2da-b8dc5d6c17f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f85f44c-d241-4ffe-90fc-32363e54d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_sequence(sequence, size):\n",
    "  a = np.zeros(size)\n",
    "  g = np.zeros(size)\n",
    "  c = np.zeros(size)\n",
    "  t = np.zeros(size)\n",
    "  for i, chr in enumerate(sequence):\n",
    "    if chr.upper() == 'A':\n",
    "      a[i] = 1\n",
    "    if chr.upper() == 'G':\n",
    "      g[i] = 1\n",
    "    if chr.upper() == 'C':\n",
    "      c[i] = 1\n",
    "    if chr.upper() == 'T':\n",
    "      t[i] = 1\n",
    "\n",
    "  return a, g, c, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6ac7b-e2e9-4e7b-9d70-648dbd500e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca53f2f6-7c84-436d-97aa-30556d8e8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "inputs = []\n",
    "\n",
    "for i, row in maverick.iterrows():\n",
    "  mav_start = row['start']\n",
    "  mav_end = row['end']\n",
    "\n",
    "  chr_row = full_data[full_data['chromosome'] == row['chromosome']]\n",
    "  if chr_row.empty:\n",
    "    continue\n",
    "\n",
    "  chr_seq = chr_row.iloc[0]['sequence']\n",
    "\n",
    "  if row['length'] < size:\n",
    "    padding_length = size - row['length']\n",
    "    padding_left = random.randrange(0, padding_length)\n",
    "    padding_right = padding_length - padding_left\n",
    "\n",
    "    start = mav_start - padding_left\n",
    "    end = mav_end + padding_right + 1\n",
    "\n",
    "    mask = np.zeros(size)\n",
    "    mask[padding_left:row['length']] = 1\n",
    "\n",
    "    # test\n",
    "    if chr_seq[start+padding_left-1:end-(padding_right+1)] != row['sequence']:\n",
    "      print(f\"{i}th input is wrong.\")\n",
    "      print(f\"seq_start: {chr_seq[start+padding_left+1:start+padding_left+10]}, actual: {row['sequence'][0:10]}\")\n",
    "      print(f\"seq_end: {chr_seq[end-padding_right-10:end-(padding_right+2)]}, actual: {row['sequence'][-10:-1]}\")\n",
    "  else:\n",
    "    start = mav_start\n",
    "    end = mav_start + size\n",
    "\n",
    "    mask = np.ones(size)\n",
    "\n",
    "  inputs.append(one_hot_sequence(chr_seq[start:end], size))\n",
    "  \n",
    "  masks.append(mask)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c385c352-cf7f-477a-9fa3-56ef9088599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'x': inputs, 'targets': masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7a80f64-1149-449a-988e-db0522a0232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cells below so that the masks and targets\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a346153-64f1-42f4-96eb-170bfa199f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(f\"./mav_data1.pkl\", 'rb')\n",
    "\n",
    "data1 = pickle.load(file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f5008f-e467-44a3-94ac-0db87cad6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(f\"./mav_data2.pkl\", 'rb')\n",
    "\n",
    "data2 = pickle.load(file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa32d354-d4bc-48ff-9734-51447ef7d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['targets'] = data1['targets'] + data2['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6187accb-094d-40c8-aebb-dee84b51fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor(np.asarray(data2['targets'])).float()\n",
    "d = torch.tensor(np.asarray(data2['x'])).float()\n",
    "d = d.reshape(d.shape[0], 1, d.shape[1], d.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c728976-85ff-4374-8ac7-f1711ea99a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_data = {'x': npinputs, 'targets': npmasks}\n",
    "\n",
    "# file = open(f\"./np_mav_data.pkl\", 'wb')\n",
    "\n",
    "# pickle.dump(np_data, file)\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31f3ebc-11cc-49f4-89a7-fb53ea80d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25\n",
    "\n",
    "dataset = pydata.TensorDataset(d, target)\n",
    "train_data, valid_data = pydata.random_split(dataset, [0.7, 0.3])\n",
    "train_dl = pydata.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dl = pydata.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7153e6-7bc8-432b-b62f-982928047eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAVDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c1 = self.contract_block(1, 24, 3, (2, 1))\n",
    "        self.c2 = self.contract_block(24, 48, 3, 1)\n",
    "        self.c3 = self.contract_block(48, 96, 3, 1)\n",
    "\n",
    "        self.ex1 = self.expand_block(96, 48, 3, 1)\n",
    "        self.ex2 = self.expand_block(48*2, 24, 3, 1)\n",
    "        self.ex3 = self.expand_block(24*2, 1, 3, (2, 1), 2, (0, 1))\n",
    "\n",
    "        self.fl = torch.nn.MaxPool2d(kernel_size=(15, 1), stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.c1(x)\n",
    "        conv2 = self.c2(conv1)\n",
    "        conv3 = self.c3(conv2)\n",
    "\n",
    "        upconv1 = self.ex1(conv3)\n",
    "        upconv2 = self.ex2(torch.cat([upconv1, conv2], 1))\n",
    "        upconv3 = self.ex3(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "        out = self.fl(upconv3)\n",
    "        return out\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding, output_stride=2, output_padding=1):\n",
    "\n",
    "        expand = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=output_stride, padding=1, output_padding=output_padding) \n",
    "                                )\n",
    "        return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab02f9b-5245-4105-abb4-a028ee2d2059",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_78/1606250530.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./ltr_detector.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# detector.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         loaded_storages[key] = torch.storage.TypedStorage(\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m             dtype=dtype)\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# detector = torch.load('./ltr_detector.pt')\n",
    "# # detector.cuda()\n",
    "# print(detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aea27e4-c61d-4bad-9297-8bc0959bb1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAVDetector(\n",
      "  (c1): Sequential(\n",
      "    (0): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1))\n",
      "    (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (c2): Sequential(\n",
      "    (0): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (c3): Sequential(\n",
      "    (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ex1): Sequential(\n",
      "    (0): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      "  (ex2): Sequential(\n",
      "    (0): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      "  (ex3): Sequential(\n",
      "    (0): Conv2d(48, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1))\n",
      "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1))\n",
      "    (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(0, 1))\n",
      "  )\n",
      "  (fl): MaxPool2d(kernel_size=(15, 1), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "detector = MAVDetector()\n",
    "# detector.cuda()\n",
    "print(detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d34b7529-21bd-4b62-a9d9-e6b6617a9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_metric = metrics.JaccardIndex(task='binary')\n",
    "# iou_metric.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a149724-55eb-488e-98da-14b07fd2fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for output, stats, and figures\n",
    "vexample = {}\n",
    "texample = {}\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "val_iou = []\n",
    "train_iou = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900b2e0c-f8b2-4abf-a7b6-7e5a4916e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, save_batch=False):\n",
    "    epoch_loss = 0\n",
    "    epoch_iou = 0\n",
    "    epoch_sum = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (x, y) in iterator:\n",
    "        # x = x.cuda()\n",
    "        # y = y.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.reshape(y.shape[0], y.shape[1])\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "        iou_score = iou_metric(torch.round(y_pred), y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_iou += iou_score.item()\n",
    "        epoch_sum += (torch.sum(y_pred).item() / y_pred.shape[0])\n",
    "        if save_batch:\n",
    "          texample['prediction'] = torch.round(y_pred)\n",
    "          texample['y'] = y\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_iou / len(iterator), epoch_sum / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "182b2c0c-bae7-435c-922a-87a688891f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, iterator, criterion, save_batch=False):\n",
    "    epoch_loss = 0\n",
    "    epoch_iou = 0\n",
    "    epoch_sum = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "       for (x, y) in iterator:\n",
    "          # x = x.cuda()\n",
    "          # y = y.cuda()\n",
    "          \n",
    "          y_pred = model(x)\n",
    "          y_pred = y_pred.reshape(y.shape[0], y.shape[1])\n",
    "\n",
    "          loss = criterion(y_pred, y)\n",
    "          iou_score = iou_metric(torch.round(y_pred), y)\n",
    "\n",
    "          epoch_loss += loss.item()\n",
    "          epoch_iou += iou_score.item()\n",
    "          epoch_sum += (torch.sum(y_pred).item() / y_pred.shape[0])\n",
    "          if save_batch:\n",
    "            vexample['prediction'] = torch.round(y_pred)\n",
    "            vexample['y'] = y\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_iou / len(iterator), epoch_sum / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f8126-e0ff-4351-aa39-4f4d75e4f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "best_validation_loss = float(\"inf\")\n",
    "optimizer = optim.Adam(detector.parameters(), lr = 1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    save_batch = epoch+1 == EPOCHS\n",
    "    t_loss, t_iou, t_sum = train(detector, train_dl, optimizer, criterion, save_batch=save_batch)\n",
    "    v_loss, v_iou, v_sum = validate(detector, valid_dl, criterion, save_batch=save_batch)\n",
    "\n",
    "    # if epoch % 1 == 0:\n",
    "    print(f'Train(Epoch{epoch+1}): loss = {t_loss} | iou = {t_iou} | sum = {t_sum}')\n",
    "    print(f'Validate(Epoch{epoch+1}): loss = {v_loss} | iou = {v_iou} | sum = {v_sum}')\n",
    "    print(' --- ')\n",
    "\n",
    "    train_loss.append(t_loss)\n",
    "    val_loss.append(v_loss)\n",
    "    train_iou.append(t_iou)\n",
    "    val_iou.append(v_iou)\n",
    "\n",
    "    if epoch > 0 and best_validation_loss > v_loss:\n",
    "        best_validation_loss = v_loss\n",
    "        torch.save(detector, './ltr_detector.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
